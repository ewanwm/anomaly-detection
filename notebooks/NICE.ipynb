{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import normflows as nf\n",
    "from normflows import nets\n",
    "from normflows.flows import Planar, Radial, MaskedAffineFlow, BatchNorm\n",
    "\n",
    "import uproot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import corner\n",
    "\n",
    "import typing\n",
    "\n",
    "import anomalydetector\n",
    "from anomalydetector.models.VAE import NFVAE, VAE\n",
    "from anomalydetector.models.NICE import NICEModel, NICE_gaussian_loss\n",
    "from anomalydetector.processing import InMemoryND280EventDataset, nd280EventDataset\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Found cuda device, will use GPU\")\n",
    "else:\n",
    "    print(\"No GPU :(\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.open('/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2air_v1.root') as file:\n",
    "\n",
    "    sample_sum = file['sample_sum'].arrays(\n",
    "        filter_branch=lambda b: b.name.find(\"Graph\") == -1 and b.typename.find(\"std::vector\") == -1,\n",
    "        library='pd'\n",
    "    )\n",
    "\n",
    "    print (\":::::: Available branches ::::::::\")\n",
    "\n",
    "    for branch in file['sample_sum'].branches:\n",
    "        print(f'  - {branch.name}: {branch.typename}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = InMemoryND280EventDataset(\n",
    "    #root=\"/home/hep/ewmiller/anomaly-detection/processed_files/MC/ID\",\n",
    "    filenames=[\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run3air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run5water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run6air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run7water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run9water_v1.root'\n",
    "    ],\n",
    "    branches=[\"Pmu\", \"CosThetamu\", \"RecoPiMom\", \"RecoPiDirX\", \"RecoPiDirY\", \"RecoPiDirZ\", \"RecoProtonMom\", \"RecoProtonDirX\", \"RecoProtonDirY\", \"RecoProtonDirZ\"],\n",
    "    branch_scaling=np.array([0.2e-3, 1.0, 0.2e-3, 1.0, 1.0, 1.0, 0.2e-3, 1.0, 1.0, 1.0], dtype=np.float32),\n",
    "    branch_mask_vals=np.array([-999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0]),\n",
    "    branch_mask_replace_vals=np.array([0.0, -1.0, 0.0, -2.0, -2.0, -2.0, 0.0, -2.0, -2.0, -2.0], dtype=np.float32),\n",
    "    filter=\"(isSRC!=1) & (RecoProton_Topo>0) & (RecoPi_Topo>0)\" #\"q0<2000.0\"\n",
    ")\n",
    "\n",
    "ood_ds = InMemoryND280EventDataset(\n",
    "    #root=\"/home/hep/ewmiller/anomaly-detection/processed_files/MC/OOD\",\n",
    "    filenames=[\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run3air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run5water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run6air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run7water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run9water_v1.root'\n",
    "    ],\n",
    "    branches=[\"Pmu\", \"CosThetamu\", \"RecoPiMom\", \"RecoPiDirX\", \"RecoPiDirY\", \"RecoPiDirZ\", \"RecoProtonMom\", \"RecoProtonDirX\", \"RecoProtonDirY\", \"RecoProtonDirZ\"],\n",
    "    branch_scaling=np.array([0.2e-3, 1.0, 0.2e-3, 1.0, 1.0, 1.0, 0.2e-3, 1.0, 1.0, 1.0], dtype=np.float32),\n",
    "    branch_mask_vals=np.array([-999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0]),\n",
    "    branch_mask_replace_vals=np.array([0.0, -1.0, 0.0, -2.0, -2.0, -2.0, 0.0, -2.0, -2.0, -2.0], dtype=np.float32),\n",
    "    filter=\"(isSRC==1) & (RecoProton_Topo>0) & (RecoPi_Topo>0)\" #\"q0>= 2000.0\"\n",
    ")\n",
    "\n",
    "train_ds.process()\n",
    "ood_ds.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_ds)}')\n",
    "print(f'Number of OOD examples:      {len(ood_ds)}')\n",
    "\n",
    "\n",
    "raw_dist = np.ndarray((1, train_ds.get_n_features()))\n",
    "ood_dist = np.ndarray((1, ood_ds.get_n_features()))\n",
    "\n",
    "print()\n",
    "print(\"##### Making ID and OOD raw distributions ######\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4096, shuffle=True)\n",
    "progressbar = tqdm(enumerate(train_loader), total=len(train_loader)-1)\n",
    "for batch_n, (x, n) in progressbar:\n",
    "    raw_dist = np.concatenate((raw_dist, x), axis=0)\n",
    "    progressbar.update()\n",
    "\n",
    "ood_loader = DataLoader(ood_ds, batch_size=4096, shuffle=True)\n",
    "progressbar = tqdm(enumerate(ood_loader), total=len(ood_loader)-1)\n",
    "for batch_n, (x, n) in progressbar:\n",
    "    ood_dist = np.concatenate((ood_dist, x), axis=0)\n",
    "    progressbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "def make_corner_plot(id_dist, ood_dist, ranges, titles, n_bins=20):\n",
    "    figure = corner.corner(\n",
    "        id_dist, \n",
    "        range  = ranges, \n",
    "        titles = titles,\n",
    "        title_fmt = None,\n",
    "        hist_kwargs={\"color\": 'tab:green', \"alpha\": 0.3, \"fill\": True, },\n",
    "        color = \"tab:green\",\n",
    "        weights = np.ones(raw_dist.shape[0]) / raw_dist.shape[0],\n",
    "        show_titles = True,\n",
    "        plot_contours = True,\n",
    "        plot_datapoints = False,\n",
    "        fill_contours=True,\n",
    "        plot_density=False,\n",
    "        density = True,\n",
    "        hist2d_kwargs = {\"no_fill_contours\": True, \"plot_datapoints\": False},\n",
    "        hist1d_kwargs = {\"density\": True},\n",
    "        bins = n_bins,\n",
    "        quiet = True,\n",
    "    )\n",
    "\n",
    "    corner.corner(\n",
    "        ood_dist, \n",
    "        range  = ranges,\n",
    "        titles = titles,\n",
    "        title_fmt = None,\n",
    "        hist_kwargs={\"color\": 'tab:orange', \"alpha\": 0.3, \"fill\": True, },\n",
    "        color=\"tab:orange\",\n",
    "        weights = np.ones(ood_dist.shape[0]) / ood_dist.shape[0],\n",
    "        show_titles = True,\n",
    "        plot_contours = True,\n",
    "        fill_contours = True,\n",
    "        plot_datapoints = False,\n",
    "        plot_density=False,\n",
    "        density=True,\n",
    "        hist2d_kwargs = {\"no_fill_contours\": True, \"plot_datapoints\": False, \"new_fig\": False},\n",
    "        hist1d_kwargs = {\"density\": True},\n",
    "        fig = figure,\n",
    "        bins = n_bins, \n",
    "        quiet = True,\n",
    "    )\n",
    "\n",
    "    id_line = mlines.Line2D([], [], color=\"tab:green\", label=\"In Distribution (ID)\")\n",
    "    ood_line = mlines.Line2D([], [], color=\"tab:orange\", label='Out of Distribution (OOD)')\n",
    "\n",
    "    figure.legend(handles=[id_line, ood_line], bbox_to_anchor=(0., 0.8, 0.9, .0), loc=4, fontsize=24)\n",
    "\n",
    "    return figure\n",
    "\n",
    "make_corner_plot(raw_dist, ood_dist, \n",
    "    ranges = [(-0.2, 1.5), (0.0,1.1),   (-0.2, 1.5), (-1.1,1.1),   (-1.1, 1.1),  (0.0, 1.0),   (-0.2, 1.5), (-1.1,1.1),  (-1.1, 1.1), (0.0, 1.1)],\n",
    "    titles = [\"p mu\",      \"ctheta mu\", \"p pi\",      \"pi dir [x]\", \"pi dir [y]\", \"pi dir [z]\", \"p p\",       \"p dir [x]\", \"p dir[y]\",  \"p dir[z]\"],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_dists(model, data_loader, quiet=False):\n",
    "    \n",
    "    progressbar = tqdm(enumerate(data_loader), total=len(data_loader), desc = \"loading data\", leave=False, disable=quiet) \n",
    "\n",
    "    enc_dist = np.ndarray((1, n_features))\n",
    "    llh_dist = np.ndarray((1, 1))\n",
    "\n",
    "    if isinstance(model, nf.NormalizingFlowVAE):\n",
    "        for batch_n, (x, n) in progressbar:\n",
    "            x = x.to(device)\n",
    "            encoded, log_q, log_p = model(x, num_samples)\n",
    "            decoded = model.decoder(encoded)\n",
    "            plt.scatter(x[:,0].detach(),decoded[:,0,0].detach(), c=[0.2, 0.4, 0.8, 0.3], s=0.2)\n",
    "\n",
    "\n",
    "    if isinstance(model, VAE):\n",
    "        for batch_n, (x, n) in progressbar:\n",
    "            x = x.to(device)\n",
    "            encoded = model.encode(x)\n",
    "            plt.scatter(x[:, 1].detach(), encoded[:, 1].detach(), c=[0.2, 0.4, 0.8, 0.3], s=0.2)\n",
    "\n",
    "\n",
    "    if isinstance(model, NICEModel):\n",
    "\n",
    "        for batch_n, (x, n) in progressbar:\n",
    "            x = x.to(device)\n",
    "            encoded = model(x)\n",
    "            enc_dist = np.concatenate((enc_dist, encoded.detach().to(torch.device(\"cpu\"))), axis=0)\n",
    "            llh_dist = np.concatenate((llh_dist, NICE_gaussian_loss(encoded.detach().to(torch.device(\"cpu\")), model.scaling_diag.detach().to(torch.device(\"cpu\")), keepdim=True)), axis=0)\n",
    "\n",
    "    return enc_dist, llh_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train_ds.get_n_features()\n",
    "\n",
    "######################\n",
    "## set up model ##\n",
    "######################\n",
    "\n",
    "# model = VAE(6, 2) \n",
    "\n",
    "# model = NFVAE(\n",
    "#     n_bottleneck = 2,\n",
    "#     hidden_units_encoder = [n_features, 8, 4, n_bottleneck*2],\n",
    "#     hidden_units_decoder = [n_bottleneck, 4, 8, n_features],\n",
    "#     n_flows = 0,\n",
    "#     flow_type = \"Planar\",\n",
    "#     device = device,\n",
    "# )\n",
    "\n",
    "model = NICEModel(n_features, 5, [64, 128, 256, 128, 64])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "model.compile()\n",
    "model.train()\n",
    "\n",
    "print(\"##### Model #####'\")\n",
    "for module in model.modules():\n",
    "    print(module)\n",
    "\n",
    "print(f\"trainable parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300\n",
    "\n",
    "train_loader:DataLoader = DataLoader(train_ds, batch_size=100000, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "epoch_progressbar = tqdm(range(n_epochs), total = n_epochs, desc=\"epoch\")\n",
    "for epoch_n in epoch_progressbar: \n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    n_batches  = 0\n",
    "\n",
    "    batch_progressbar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for batch_n, (x, n) in batch_progressbar:\n",
    "\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.train_batch(x)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_progressbar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "        # update running mean loss\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    \n",
    "    # make plot of the latent space\n",
    "    id_encoded,  id_llh  = get_latent_dists(model, train_loader, quiet=True)\n",
    "    ood_encoded, ood_llh = get_latent_dists(model, ood_loader, quiet=True)\n",
    "\n",
    "    fig = make_corner_plot(id_encoded, ood_encoded,\n",
    "        ranges = [(-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5)],\n",
    "        titles = None,\n",
    "        n_bins = 40\n",
    "    )\n",
    "\n",
    "    fig.savefig(f\"plots/latent_dist-epoch-{epoch_n:04}.png\")\n",
    "    fig.clf()\n",
    "        \n",
    "    epoch_loss /= n_batches    \n",
    "    epoch_progressbar.set_description(f\"epoch {epoch_n} loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/src_detector_5-layer-64_128_256_128_64.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_encoded,  id_llh  = get_latent_dists(model, train_loader)\n",
    "ood_encoded, ood_llh = get_latent_dists(model, ood_loader)\n",
    "\n",
    "plt.hist([ood_llh[:,0], id_llh[:,0],], color = [\"tab:orange\", \"tab:green\"], bins=100, range=(-50, 5), histtype=\"step\", label=[\"OOD\", \"ID\"], fill=False)\n",
    "plt.title(\"ID vs OOD LLH Distribution\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist([ood_llh[:,0], id_llh[:,0]], color = [\"tab:orange\", \"tab:green\"], bins=50, range=(-50, 5), histtype='step', label=[\"OOD\", \"IID\"], density=True, fill=False)\n",
    "plt.title(\"ID vs OOD LLH Distribution - Normalised\")\n",
    "plt.xlabel(\"LLH\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "LLH_CUT = 5.0\n",
    "feat_names = [\"p mu\", \"ctheta mu\", \"p pi\", \"ctheta pi\", \"p p\", \"ctheta p\", \"reco N pi\", \"reco N Prot\"]\n",
    "feat_ranges = [(0.001, 1), (-0.999,1), (0.001, 1), (-0.999,1), (0.001, 1), (-0.999,1), (0, 1.0), (0, 1.0)]\n",
    "far_outliers = ood_llh[:, 0] < LLH_CUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_corner_plot(id_encoded, ood_encoded,\n",
    "    ranges = [(-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5)],\n",
    "    titles = None,\n",
    "    n_bins = 40\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
