{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import typing\n",
    "\n",
    "import anomalydetector\n",
    "from anomalydetector.models.VAE import NFVAE, VAE\n",
    "from anomalydetector.models.utils import get_latent_dists, summarise_model\n",
    "from anomalydetector.models.NICE import NICEModel\n",
    "from anomalydetector.processing import InMemoryND280EventDataset, nd280EventDataset\n",
    "from anomalydetector.plotting import make_corner_plot\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Found cuda device, will use GPU\")\n",
    "else:\n",
    "    print(\"No GPU :(\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Training Data\n",
    "\n",
    "Here we set up two datasets. One is our training data, for which we will take non-src events that have a reconstructed proton.\n",
    "The other will be our test out of distribution dummy test dataset, for which we will take true src events with a reconstructed proton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = InMemoryND280EventDataset(\n",
    "    #root=\"/home/hep/ewmiller/anomaly-detection/processed_files/MC/ID\",\n",
    "    filenames=[\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run3air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run5water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run6air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run7water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run9water_v1.root'\n",
    "    ],\n",
    "    branches=[\"Pmu\", \"RecoLepDirX\", \"RecoLepDirY\", \"RecoLepDirZ\", \"RecoProtonMom\", \"RecoProtonDirX\", \"RecoProtonDirY\", \"RecoProtonDirZ\"],\n",
    "    branch_scaling=np.array([0.2e-3, 1.0, 1.0, 1.0, 0.2e-3, 1.0, 1.0, 1.0], dtype=np.float32),\n",
    "    branch_mask_vals=np.array([-999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0]),\n",
    "    branch_mask_replace_vals=np.array([0.0, -2.0, -2.0, -2.0, 0.0, -2.0, -2.0, -2.0], dtype=np.float32),\n",
    "    filter=\"(isSRC!=1) & (RecoProtonMom!=-999.0)\" #\"q0<2000.0\"\n",
    ")\n",
    "\n",
    "ood_ds = InMemoryND280EventDataset(\n",
    "    #root=\"/home/hep/ewmiller/anomaly-detection/processed_files/MC/OOD\",\n",
    "    filenames=[\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run2water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run3air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run4water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run5water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run6air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run7water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8air_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run8water_v1.root',\n",
    "        '/vols/t2k/nd280-OA2022/FDS-inputs/FDS_run9water_v1.root'\n",
    "    ],\n",
    "    branches=[\"Pmu\", \"RecoLepDirX\", \"RecoLepDirY\", \"RecoLepDirZ\", \"RecoProtonMom\", \"RecoProtonDirX\", \"RecoProtonDirY\", \"RecoProtonDirZ\"],\n",
    "    branch_scaling=np.array([0.2e-3, 1.0, 1.0, 1.0, 0.2e-3, 1.0, 1.0, 1.0], dtype=np.float32),\n",
    "    branch_mask_vals=np.array([-999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0, -999.0]),\n",
    "    branch_mask_replace_vals=np.array([0.0, -2.0, -2.0, -2.0, 0.0, -2.0, -2.0, -2.0], dtype=np.float32),\n",
    "    filter=\"(isSRC==1) & (RecoProtonMom!=-999.0)\" #\"q0>= 2000.0\"\n",
    ")\n",
    "\n",
    "train_ds.dump_branches()\n",
    "\n",
    "train_ds.process()\n",
    "ood_ds.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print some info about our datasets and make plots of the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_ds)}')\n",
    "print(f'Number of OOD examples:      {len(ood_ds)}')\n",
    "\n",
    "make_corner_plot(train_ds.get_data().numpy(), ood_ds.get_data().numpy(), \n",
    "    ranges = [(-0.2, 1.5), (-1.1,1.1),    (-1.1,1.1),    (-1.1,1.1),    (-0.2, 1.5), (-1.1,1.1),  (-1.1, 1.1), (0.0, 1.1)],\n",
    "    titles = [\"p mu\",      \"lep dir [x]\", \"lep dir [y]\", \"lep dir [z]\", \"p p\",       \"p dir [x]\", \"p dir[y]\",  \"p dir[z]\"],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Model\n",
    "\n",
    "Now we create our model. Feel free to comment / uncomment and play around with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = train_ds.get_n_features()\n",
    "\n",
    "######################\n",
    "## set up model ##\n",
    "######################\n",
    "\n",
    "# model = VAE(6, 2) \n",
    "\n",
    "# model = NFVAE(\n",
    "#     n_bottleneck = 2,\n",
    "#     hidden_units_encoder = [n_features, 8, 4, n_bottleneck*2],\n",
    "#     hidden_units_decoder = [n_bottleneck, 4, 8, n_features],\n",
    "#     n_flows = 0,\n",
    "#     flow_type = \"Planar\",\n",
    "#     device = device,\n",
    "# )\n",
    "\n",
    "model = NICEModel(\n",
    "    n_features = n_features, \n",
    "    n_flows = 5, \n",
    "    n_hidden = [64, 128, 256, 128, 64]\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.compile()\n",
    "model.train()\n",
    "\n",
    "summarise_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "## make loaders for the two datasets\n",
    "train_loader:DataLoader = DataLoader(train_ds, batch_size=100000, shuffle=True)\n",
    "ood_loader:DataLoader   = DataLoader(ood_ds,   batch_size=100000, shuffle=True)\n",
    "\n",
    "epoch_progressbar = tqdm(range(n_epochs), total = n_epochs, desc=\"epoch\")\n",
    "for epoch_n in epoch_progressbar: \n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    n_batches  = 0\n",
    "\n",
    "    batch_progressbar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    for batch_n, (x, n) in batch_progressbar:\n",
    "\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.train_batch(x)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_progressbar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "        # update running mean loss\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    \n",
    "    # make plot of the latent space\n",
    "    id_encoded,  id_llh  = get_latent_dists(model, train_loader, device, quiet=True)\n",
    "    ood_encoded, ood_llh = get_latent_dists(model, ood_loader,   device, quiet=True)\n",
    "\n",
    "    fig = make_corner_plot(id_encoded, ood_encoded,\n",
    "        ranges = [(-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5)],\n",
    "        titles = None,\n",
    "        n_bins = 40\n",
    "    )\n",
    "\n",
    "    fig.savefig(f\"plots/latent_dist-epoch-{epoch_n:04}.png\")\n",
    "    fig.clear()\n",
    "    plt.close(fig)\n",
    "        \n",
    "    epoch_loss /= n_batches    \n",
    "    epoch_progressbar.set_description(f\"epoch {epoch_n} loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the model so we can use it later or pick up the training where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"NICE_src_detector_5-layer-64_128_256_128_64.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the latent distributions and likelihoods look like post-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_encoded,  id_llh  = get_latent_dists(model, train_loader, device)\n",
    "ood_encoded, ood_llh = get_latent_dists(model, ood_loader,   device)\n",
    "\n",
    "plt.hist([ood_llh[:,0], id_llh[:,0],], color = [\"tab:orange\", \"tab:green\"], bins=100, range=(-50, 5), histtype=\"step\", label=[\"OOD\", \"ID\"], fill=False)\n",
    "plt.title(\"ID vs OOD LLH Distribution\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist([ood_llh[:,0], id_llh[:,0]], color = [\"tab:orange\", \"tab:green\"], bins=50, range=(-50, 5), histtype='step', label=[\"OOD\", \"IID\"], density=True, fill=False)\n",
    "plt.title(\"ID vs OOD LLH Distribution - Normalised\")\n",
    "plt.xlabel(\"LLH\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "LLH_CUT = 5.0\n",
    "far_outliers = ood_llh[:, 0] < LLH_CUT\n",
    "\n",
    "make_corner_plot(id_encoded, ood_encoded,\n",
    "    ranges = [(-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5), (-1.5, 1.5), (-1.5,1.5)],\n",
    "    titles = None,\n",
    "    n_bins = 40\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
